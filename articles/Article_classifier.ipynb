{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix, lil_matrix\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from copy import deepcopy\n",
    "from string import punctuation\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "SEED = 42\n",
    "punct = set(punctuation) | {'‘','’','—',' ','\\t','\\n'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load_word2vec_format(\"C:/Users/Андрей/Desktop/DM/constructions/GoogleNews-vectors-negative300.bin.gz\",\n",
    "                                      binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NP</th>\n",
       "      <th>POS_tags</th>\n",
       "      <th>Head</th>\n",
       "      <th>Head_countability</th>\n",
       "      <th>NP_first_letter</th>\n",
       "      <th>Head_POS</th>\n",
       "      <th>hypernyms</th>\n",
       "      <th>higher_hypernyms</th>\n",
       "      <th>hhead</th>\n",
       "      <th>hhead_POS</th>\n",
       "      <th>deprel</th>\n",
       "      <th>prevprev</th>\n",
       "      <th>prev</th>\n",
       "      <th>post</th>\n",
       "      <th>postpost</th>\n",
       "      <th>prevprev_POS</th>\n",
       "      <th>prev_POS</th>\n",
       "      <th>post_POS</th>\n",
       "      <th>postpost_POS</th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ageism</td>\n",
       "      <td>NN</td>\n",
       "      <td>ageism</td>\n",
       "      <td></td>\n",
       "      <td>a</td>\n",
       "      <td>NN</td>\n",
       "      <td>discrimination</td>\n",
       "      <td>group_action</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>foundation</td>\n",
       "      <td>NN</td>\n",
       "      <td>foundation</td>\n",
       "      <td>both</td>\n",
       "      <td>f</td>\n",
       "      <td>NN</td>\n",
       "      <td>relation</td>\n",
       "      <td>abstraction</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>of</td>\n",
       "      <td>age</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>IN</td>\n",
       "      <td>NN</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age discrimination</td>\n",
       "      <td>NN NN</td>\n",
       "      <td>discrimination</td>\n",
       "      <td>U</td>\n",
       "      <td>a</td>\n",
       "      <td>NN</td>\n",
       "      <td>social_control</td>\n",
       "      <td>act event</td>\n",
       "      <td>foundation</td>\n",
       "      <td>NN</td>\n",
       "      <td>nmod</td>\n",
       "      <td>foundation</td>\n",
       "      <td>of</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NN</td>\n",
       "      <td>IN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>steve scrutton</td>\n",
       "      <td>NNP NNP</td>\n",
       "      <td>scrutton</td>\n",
       "      <td></td>\n",
       "      <td>s</td>\n",
       "      <td>NNP</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>steve</td>\n",
       "      <td>VB</td>\n",
       "      <td>obj</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>steve scrutton</td>\n",
       "      <td>NNP NNP</td>\n",
       "      <td>scrutton</td>\n",
       "      <td></td>\n",
       "      <td>s</td>\n",
       "      <td>NNP</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>steve</td>\n",
       "      <td>NNP</td>\n",
       "      <td>flat</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>is</td>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>VBZ</td>\n",
       "      <td>DT</td>\n",
       "      <td>zero</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   NP POS_tags            Head Head_countability  \\\n",
       "0              ageism       NN          ageism                     \n",
       "1          foundation       NN      foundation              both   \n",
       "2  age discrimination    NN NN  discrimination                 U   \n",
       "3      steve scrutton  NNP NNP        scrutton                     \n",
       "4      steve scrutton  NNP NNP        scrutton                     \n",
       "\n",
       "  NP_first_letter Head_POS       hypernyms higher_hypernyms       hhead  \\\n",
       "0               a       NN  discrimination     group_action               \n",
       "1               f       NN        relation      abstraction               \n",
       "2               a       NN  social_control        act event  foundation   \n",
       "3               s      NNP                                        steve   \n",
       "4               s      NNP                                        steve   \n",
       "\n",
       "  hhead_POS deprel    prevprev prev post postpost prevprev_POS prev_POS  \\\n",
       "0                                                                         \n",
       "1                                     of      age                         \n",
       "2        NN   nmod  foundation   of                         NN       IN   \n",
       "3        VB    obj                                                        \n",
       "4       NNP   flat                    is        a                         \n",
       "\n",
       "  post_POS postpost_POS Article  \n",
       "0                          zero  \n",
       "1       IN           NN     the  \n",
       "2                          zero  \n",
       "3                          zero  \n",
       "4      VBZ           DT    zero  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('articles.csv',delimiter=';',encoding='utf-8-sig')\n",
    "data = data.fillna('')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_vectors = lil_matrix((data.shape[0],300))\n",
    "for i,word in enumerate(data['Head']):\n",
    "    if word in model:\n",
    "        all_vectors[i,:] = model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?:^| )(.+?)(?= |$)',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../unique_words.txt','r',encoding='utf-8') as f:\n",
    "    unique_words = f.read().split('\\n')\n",
    "\n",
    "onewordvect = CountVectorizer(token_pattern='.+')\n",
    "onewordvect.fit(unique_words+list(punct))\n",
    "\n",
    "with open('../Penn_POS_tagset.txt','r',encoding='utf-8') as f:\n",
    "    unique_pos = f.read().split('\\n')\n",
    "\n",
    "pos_vect = CountVectorizer(token_pattern='(?:^| )(.+?)(?= |$)')\n",
    "pos_vect.fit(unique_pos+list(punct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = data['Article']\n",
    "data = data.drop('Article',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pres_idx = target[(target == 'a') | (target == 'an') | (target == 'the')].index\n",
    "binary_target = deepcopy(target)\n",
    "binary_target[(binary_target == 'a') | (binary_target == 'an') | (binary_target == 'the')] = 'present'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np_vect = CountVectorizer(token_pattern = '\\\\b\\\\w+\\\\b')\n",
    "npm = np_vect.fit_transform(data['NP'])\n",
    "\n",
    "pos = pos_vect.transform(data['POS_tags'])\n",
    "head_pos = pos_vect.transform(data['Head_POS'])\n",
    "#hhead_pos = pos_vect.transform(data['hhead_POS'])\n",
    "prevprev_pos = pos_vect.transform(data['prevprev_POS'])\n",
    "prev_pos = pos_vect.transform(data['prev_POS'])\n",
    "post_pos = pos_vect.transform(data['post_POS'])\n",
    "postpost_pos = pos_vect.transform(data['postpost_POS'])\n",
    "\n",
    "countability = pd.get_dummies(data['Head_countability'],drop_first=True).to_sparse()\n",
    "\n",
    "letter_vect = CountVectorizer(token_pattern='.+')\n",
    "first_letter = letter_vect.fit_transform(data['NP_first_letter'])\n",
    "\n",
    "hyp_vect = CountVectorizer()\n",
    "hyp = hyp_vect.fit_transform(data['hypernyms'])\n",
    "\n",
    "hhyp_vect = CountVectorizer()\n",
    "hhyp = hhyp_vect.fit_transform(data['higher_hypernyms'])\n",
    "\n",
    "#deprel_vect = CountVectorizer()\n",
    "#deprel = deprel_vect.fit_transform(data['deprel'])\n",
    "\n",
    "#hhead_vect = CountVectorizer(token_pattern='.+')\n",
    "#hhead = hhead_vect.fit_transform(data['hhead'])\n",
    "\n",
    "head = onewordvect.transform(data['Head'])\n",
    "prevprev = onewordvect.transform(data['prevprev'])\n",
    "prev = onewordvect.transform(data['prev'])\n",
    "post = onewordvect.transform(data['post'])\n",
    "postpost = onewordvect.transform(data['postpost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sparse = hstack((npm,pos,head,countability,first_letter,head_pos,hyp,hhyp,all_vectors,\n",
    "                      prevprev,prev,post,postpost,prevprev_pos,prev_pos,post_pos,postpost_pos)).tocsr()\n",
    "nonzero_columns = np.unique(data_sparse.nonzero()[1]) # TODO: need to remember what cols were omitted\n",
    "data_sparse = data_sparse[:,nonzero_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# presence classifier & a-an-the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_sparse, binary_target, test_size=0.33, \n",
    "                                                    random_state=SEED,stratify=binary_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(random_state=SEED)\n",
    "logit.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.854628052243\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    present       0.80      0.75      0.78      5271\n",
      "       zero       0.88      0.90      0.89     10578\n",
      "\n",
      "avg / total       0.85      0.85      0.85     15849\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_l = logit.predict(X_test)\n",
    "print(accuracy_score(y_test,pred_l))\n",
    "print(classification_report(y_test,pred_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=42, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_pres = LogisticRegression(random_state=SEED,multi_class='multinomial',solver='lbfgs')\n",
    "logit_pres.fit(X_train[np.where(y_train == 'present')[0],:],target[y_train[y_train == 'present'].index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.664593781344\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          a       0.60      0.66      0.63       892\n",
      "         an       0.60      0.48      0.53       186\n",
      "        the       0.68      0.91      0.78      2898\n",
      "       zero       0.00      0.00      0.00      1009\n",
      "\n",
      "avg / total       0.53      0.66      0.59      4985\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Андрей\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pred_l_pres = logit_pres.predict(X_test[np.where(pred_l == 'present')[0],:])\n",
    "print(accuracy_score(target[y_test[pred_l == 'present'].index],pred_l_pres))\n",
    "print(classification_report(target[y_test[pred_l == 'present'].index],pred_l_pres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_l[pred_l == 'present'] = pred_l_pres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.812795759985\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          a       0.60      0.53      0.56      1111\n",
      "         an       0.60      0.39      0.47       228\n",
      "        the       0.68      0.67      0.68      3932\n",
      "       zero       0.88      0.90      0.89     10578\n",
      "\n",
      "avg / total       0.81      0.81      0.81     15849\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(target[y_test.index],pred_l))\n",
    "print(classification_report(target[y_test.index],pred_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just in case - list of classifiers that support predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Андрей\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\Андрей\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Андрей\\Anaconda3\\lib\\site-packages\\sklearn\\lda.py:6: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n",
      "C:\\Users\\Андрей\\Anaconda3\\lib\\site-packages\\sklearn\\learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Андрей\\Anaconda3\\lib\\site-packages\\sklearn\\qda.py:6: DeprecationWarning: qda.QDA has been moved to discriminant_analysis.QuadraticDiscriminantAnalysis in 0.17 and will be removed in 0.19.\n",
      "  \"in 0.17 and will be removed in 0.19.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier\n",
      "BaggingClassifier\n",
      "BayesianGaussianMixture\n",
      "BernoulliNB\n",
      "CalibratedClassifierCV\n",
      "DPGMM\n",
      "DecisionTreeClassifier\n",
      "ExtraTreeClassifier\n",
      "ExtraTreesClassifier\n",
      "GMM\n",
      "GaussianMixture\n",
      "GaussianNB\n",
      "GaussianProcessClassifier\n",
      "GradientBoostingClassifier\n",
      "KNeighborsClassifier\n",
      "LDA\n",
      "LabelPropagation\n",
      "LabelSpreading\n",
      "LinearDiscriminantAnalysis\n",
      "LogisticRegression\n",
      "LogisticRegressionCV\n",
      "MLPClassifier\n",
      "MultinomialNB\n",
      "NuSVC\n",
      "QDA\n",
      "QuadraticDiscriminantAnalysis\n",
      "RandomForestClassifier\n",
      "SGDClassifier\n",
      "SVC\n",
      "VBGMM\n",
      "_BinaryGaussianProcessClassifierLaplace\n",
      "_ConstantPredictor\n",
      "_DPGMMBase\n",
      "_GMMBase\n",
      "_LDA\n",
      "_QDA\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.testing import all_estimators\n",
    "\n",
    "estimators = all_estimators()\n",
    "\n",
    "for name, class_ in estimators:\n",
    "    if hasattr(class_, 'predict_proba'):\n",
    "        print(name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
