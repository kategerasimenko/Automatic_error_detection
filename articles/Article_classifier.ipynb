{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix, lil_matrix\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from copy import deepcopy\n",
    "from string import punctuation\n",
    "import pickle\n",
    "\n",
    "SEED = 42\n",
    "punct = set(punctuation) | {'‘','’','—',' ','\\t','\\n'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OMG 10G THAT REMAIN IN MEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NP</th>\n",
       "      <th>POS_tags</th>\n",
       "      <th>Head</th>\n",
       "      <th>Head_countability</th>\n",
       "      <th>NP_first_letter</th>\n",
       "      <th>Head_POS</th>\n",
       "      <th>hypernyms</th>\n",
       "      <th>higher_hypernyms</th>\n",
       "      <th>prev_2</th>\n",
       "      <th>prev_1</th>\n",
       "      <th>prev_2_POS</th>\n",
       "      <th>prev_1_POS</th>\n",
       "      <th>post_1</th>\n",
       "      <th>post_2</th>\n",
       "      <th>post_1_POS</th>\n",
       "      <th>post_2_POS</th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>factsheet</td>\n",
       "      <td>NN</td>\n",
       "      <td>factsheet</td>\n",
       "      <td></td>\n",
       "      <td>f</td>\n",
       "      <td>NN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>what</td>\n",
       "      <td>is</td>\n",
       "      <td>WDT</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aids</td>\n",
       "      <td>NNP</td>\n",
       "      <td>aids</td>\n",
       "      <td>both</td>\n",
       "      <td>a</td>\n",
       "      <td>NNP</td>\n",
       "      <td>infectious_disease immunodeficiency</td>\n",
       "      <td>disorder disease</td>\n",
       "      <td>what</td>\n",
       "      <td>is</td>\n",
       "      <td>WDT</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>?</td>\n",
       "      <td></td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aids</td>\n",
       "      <td>NN</td>\n",
       "      <td>aids</td>\n",
       "      <td>both</td>\n",
       "      <td>a</td>\n",
       "      <td>NN</td>\n",
       "      <td>infectious_disease immunodeficiency</td>\n",
       "      <td>disorder disease</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>(</td>\n",
       "      <td>acquired</td>\n",
       "      <td>NN</td>\n",
       "      <td>VBN</td>\n",
       "      <td>zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>immune deficiency syndrome</td>\n",
       "      <td>NNP NNP NN</td>\n",
       "      <td>syndrome</td>\n",
       "      <td>C</td>\n",
       "      <td>i</td>\n",
       "      <td>NN</td>\n",
       "      <td>complex</td>\n",
       "      <td>concept</td>\n",
       "      <td>(</td>\n",
       "      <td>acquired</td>\n",
       "      <td>NN</td>\n",
       "      <td>VBN</td>\n",
       "      <td>)</td>\n",
       "      <td>is</td>\n",
       "      <td>NN</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>condition</td>\n",
       "      <td>NN</td>\n",
       "      <td>condition</td>\n",
       "      <td>C</td>\n",
       "      <td>c</td>\n",
       "      <td>NN</td>\n",
       "      <td>state</td>\n",
       "      <td>abstraction</td>\n",
       "      <td>)</td>\n",
       "      <td>is</td>\n",
       "      <td>NN</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>caused</td>\n",
       "      <td>by</td>\n",
       "      <td>VBN</td>\n",
       "      <td>IN</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           NP    POS_tags       Head Head_countability  \\\n",
       "0                   factsheet          NN  factsheet                     \n",
       "1                        aids         NNP       aids              both   \n",
       "2                        aids          NN       aids              both   \n",
       "3  immune deficiency syndrome  NNP NNP NN   syndrome                 C   \n",
       "4                   condition          NN  condition                 C   \n",
       "\n",
       "  NP_first_letter Head_POS                            hypernyms  \\\n",
       "0               f       NN                                        \n",
       "1               a      NNP  infectious_disease immunodeficiency   \n",
       "2               a       NN  infectious_disease immunodeficiency   \n",
       "3               i       NN                              complex   \n",
       "4               c       NN                                state   \n",
       "\n",
       "   higher_hypernyms prev_2    prev_1 prev_2_POS prev_1_POS  post_1    post_2  \\\n",
       "0                                                             what        is   \n",
       "1  disorder disease   what        is        WDT        VBZ       ?             \n",
       "2  disorder disease                                              (  acquired   \n",
       "3           concept      (  acquired         NN        VBN       )        is   \n",
       "4       abstraction      )        is         NN        VBZ  caused        by   \n",
       "\n",
       "  post_1_POS post_2_POS Article  \n",
       "0        WDT        VBZ    zero  \n",
       "1          .               zero  \n",
       "2         NN        VBN    zero  \n",
       "3         NN        VBZ    zero  \n",
       "4        VBN         IN       a  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../article_table/articles.csv',delimiter=';',encoding='utf-8-sig',keep_default_na=False)\n",
    "data.drop(['Sentence','raw_NP','Start_idx','Sent_start_idx'],axis=1,inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='.+', tokenizer=None,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../unique_words.txt','r',encoding='utf-8') as f:\n",
    "    unique_words = f.read().split('\\n')\n",
    "\n",
    "onewordvect = CountVectorizer(token_pattern='.+')\n",
    "onewordvect.fit(unique_words+list(punct))\n",
    "\n",
    "with open('../Penn_POS_tagset.txt','r',encoding='utf-8') as f:\n",
    "    unique_pos = f.read().split('\\n')\n",
    "\n",
    "pos_vect = CountVectorizer(token_pattern='(?:^| )(.+?)(?= |$)')\n",
    "pos_vect.fit(unique_pos+list(punct))\n",
    "\n",
    "count_vect = CountVectorizer(token_pattern='.+')\n",
    "count_vect.fit(['C','U','both','proper'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['Article']\n",
    "data.drop('Article',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pres_idx = target[(target == 'a') | (target == 'an') | (target == 'the')].index\n",
    "binary_target = deepcopy(target)\n",
    "binary_target[(binary_target == 'a') | (binary_target == 'an') | (binary_target == 'the')] = 'present'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('target.pickle','wb') as f:\n",
    "    pickle.dump(target,f)\n",
    "\n",
    "with open('binary_target.pickle','wb') as f:\n",
    "    pickle.dump(binary_target,f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n#deprel_vect = CountVectorizer()\\n#deprel = deprel_vect.fit_transform(data['deprel'])\\n\\n#hhead_vect = CountVectorizer(token_pattern='.+')\\n#hhead = hhead_vect.fit_transform(data['hhead'])\\n\\nhead = onewordvect.transform(data['Head'])\\nprevs = hstack([onewordvect.transform(data['prev_'+str(i)]) for i in range(1,3)])\\nposts = hstack([onewordvect.transform(data['post_'+str(i)]) for i in range(1,3)])\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_vect = CountVectorizer(token_pattern = '\\\\b\\\\w+\\\\b')\n",
    "npm = np_vect.fit_transform(data['NP'])\n",
    "\n",
    "'''\n",
    "pos = pos_vect.transform(data['POS_tags'])\n",
    "head_pos = pos_vect.transform(data['Head_POS'])\n",
    "#hhead_pos = pos_vect.transform(data['hhead_POS'])\n",
    "prevs_pos = hstack([pos_vect.transform(data['prev_'+str(i)+'_POS']) for i in range(1,3)])\n",
    "posts_pos = hstack([pos_vect.transform(data['post_'+str(i)+'_POS']) for i in range(1,3)])\n",
    "\n",
    "\n",
    "countability = count_vect.transform(data['Head_countability'])\n",
    "'''\n",
    "\n",
    "letter_vect = CountVectorizer(token_pattern='.+')\n",
    "first_letter = letter_vect.fit_transform(data['NP_first_letter'])\n",
    "\n",
    "hyp_vect = CountVectorizer()\n",
    "hyp = hyp_vect.fit_transform(data['hypernyms'])\n",
    "\n",
    "hhyp_vect = CountVectorizer()\n",
    "hhyp = hhyp_vect.fit_transform(data['higher_hypernyms'])\n",
    "\n",
    "'''\n",
    "\n",
    "#deprel_vect = CountVectorizer()\n",
    "#deprel = deprel_vect.fit_transform(data['deprel'])\n",
    "\n",
    "#hhead_vect = CountVectorizer(token_pattern='.+')\n",
    "#hhead = hhead_vect.fit_transform(data['hhead'])\n",
    "\n",
    "head = onewordvect.transform(data['Head'])\n",
    "prevs = hstack([onewordvect.transform(data['prev_'+str(i)]) for i in range(1,3)])\n",
    "posts = hstack([onewordvect.transform(data['post_'+str(i)]) for i in range(1,3)])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/one_word_vectorizer.pickle','wb') as f:\n",
    "    pickle.dump(onewordvect,f)\n",
    "    \n",
    "with open('../models/pos_vectorizer.pickle','wb') as f:\n",
    "    pickle.dump(pos_vect,f)\n",
    "\n",
    "with open('../models/letter_vectorizer.pickle','wb') as f:\n",
    "    pickle.dump(letter_vect,f)\n",
    "\n",
    "with open('../models/np_vectorizer.pickle','wb') as f:\n",
    "    pickle.dump(np_vect,f)\n",
    "    \n",
    "with open('../models/noun_hypernym_vectorizer.pickle','wb') as f:\n",
    "    pickle.dump(hyp_vect,f)\n",
    "    \n",
    "with open('../models/noun_higher_hypernym_vectorizer.pickle','wb') as f:\n",
    "    pickle.dump(hhyp_vect,f)\n",
    "    \n",
    "with open('../models/countability_vectorizer.pickle','wb') as f:\n",
    "    pickle.dump(count_vect,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('npm.pickle','wb') as f:\n",
    "    pickle.dump(npm,f)\n",
    "    \n",
    "with open('pos.pickle','wb') as f:\n",
    "    pickle.dump(pos,f)\n",
    "    \n",
    "with open('head.pickle','wb') as f:\n",
    "    pickle.dump(head,f)\n",
    "    \n",
    "with open('countability.pickle','wb') as f:\n",
    "    pickle.dump(countability,f)\n",
    "    \n",
    "with open('first_letter.pickle','wb') as f:\n",
    "    pickle.dump(first_letter,f)\n",
    "    \n",
    "with open('head_pos.pickle','wb') as f:\n",
    "    pickle.dump(head_pos,f)\n",
    "    \n",
    "with open('hyp.pickle','wb') as f:\n",
    "    pickle.dump(hyp,f)\n",
    "    \n",
    "with open('hhyp.pickle','wb') as f:\n",
    "    pickle.dump(hhyp,f)\n",
    "    \n",
    "with open('prevs.pickle','wb') as f:\n",
    "    pickle.dump(prevs,f)\n",
    "    \n",
    "with open('posts.pickle','wb') as f:\n",
    "    pickle.dump(posts,f)\n",
    "\n",
    "with open('prevs_pos.pickle','wb') as f:\n",
    "    pickle.dump(prevs_pos,f)\n",
    "\n",
    "with open('posts_pos.pickle','wb') as f:\n",
    "    pickle.dump(posts_pos,f)\n",
    "    \n",
    "with open('head_col.pickle','wb') as f:\n",
    "    pickle.dump(head_col,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTERMEDIATE STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('npm.pickle','rb') as f:\n",
    "    npm = pickle.load(f)\n",
    "    \n",
    "with open('pos.pickle','rb') as f:\n",
    "    pos = pickle.load(f)\n",
    "    \n",
    "with open('head.pickle','rb') as f:\n",
    "    head = pickle.load(f)\n",
    "    \n",
    "with open('countability.pickle','rb') as f:\n",
    "    countability = pickle.load(f)\n",
    "    \n",
    "with open('first_letter.pickle','rb') as f:\n",
    "    first_letter = pickle.load(f)\n",
    "    \n",
    "with open('head_pos.pickle','rb') as f:\n",
    "    head_pos = pickle.load(f)\n",
    "    \n",
    "with open('hyp.pickle','rb') as f:\n",
    "    hyp = pickle.load(f)\n",
    "    \n",
    "with open('hhyp.pickle','rb') as f:\n",
    "    hhyp = pickle.load(f)\n",
    "    \n",
    "with open('prevs.pickle','rb') as f:\n",
    "    prevs = pickle.load(f)\n",
    "    \n",
    "with open('posts.pickle','rb') as f:\n",
    "    posts = pickle.load(f)\n",
    "\n",
    "with open('prevs_pos.pickle','rb') as f:\n",
    "    prevs_pos = pickle.load(f)\n",
    "\n",
    "with open('posts_pos.pickle','rb') as f:\n",
    "    posts_pos = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sparse = hstack((npm,pos,head,countability,first_letter,head_pos,hyp,hhyp,\n",
    "                      prevs,posts,prevs_pos,posts_pos)).tocsr()\n",
    "nonzero_columns = np.unique(data_sparse.nonzero()[1]) # TODO: need to remember what cols were omitted\n",
    "data_sparse = data_sparse[:,nonzero_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21408958, 1319378)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sparse_data.pickle','wb') as f:\n",
    "    pickle.dump(data_sparse,f)\n",
    "\n",
    "with open('nonzero_columns.pickle','wb') as f:\n",
    "    pickle.dump(nonzero_columns,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# presence classifier & a-an-the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sparse_data.pickle','rb') as f:\n",
    "    data_sparse = pickle.load(f)\n",
    "\n",
    "with open('target.pickle','rb') as f:\n",
    "    target = pickle.load(f)\n",
    "\n",
    "with open('binary_target.pickle','rb') as f:\n",
    "    binary_target = pickle.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_sparse, binary_target, test_size=0.33, \n",
    "                                                    random_state=SEED,stratify=binary_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logit = LogisticRegression(random_state=SEED)\n",
    "logit.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.853807811218\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    present       0.80      0.75      0.77      5271\n",
      "       zero       0.88      0.90      0.89     10578\n",
      "\n",
      "avg / total       0.85      0.85      0.85     15849\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_l = logit.predict(X_test)\n",
    "print(accuracy_score(y_test,pred_l))\n",
    "print(classification_report(y_test,pred_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logit_pres = LogisticRegression(random_state=SEED,multi_class='multinomial',solver='lbfgs')\n",
    "logit_pres.fit(X_train[np.where(y_train == 'present')[0],:],target[y_train[y_train == 'present'].index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.66291683407\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          a       0.60      0.65      0.62       887\n",
      "         an       0.58      0.46      0.51       184\n",
      "        the       0.68      0.91      0.78      2895\n",
      "       zero       0.00      0.00      0.00      1012\n",
      "\n",
      "avg / total       0.52      0.66      0.58      4978\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC1\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pred_l_pres = logit_pres.predict(X_test[np.where(pred_l == 'present')[0],:])\n",
    "print(accuracy_score(target[y_test[pred_l == 'present'].index],pred_l_pres))\n",
    "print(classification_report(target[y_test[pred_l == 'present'].index],pred_l_pres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_l[pred_l == 'present'] = pred_l_pres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81178623257\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          a       0.60      0.52      0.56      1111\n",
      "         an       0.58      0.37      0.45       228\n",
      "        the       0.68      0.67      0.68      3932\n",
      "       zero       0.88      0.90      0.89     10578\n",
      "\n",
      "avg / total       0.81      0.81      0.81     15849\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(target[y_test.index],pred_l))\n",
    "print(classification_report(target[y_test.index],pred_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit and save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sparse_data.pickle','rb') as f:\n",
    "    data_sparse = pickle.load(f)\n",
    "    \n",
    "with open('target.pickle','rb') as f:\n",
    "    target = pickle.load(f)\n",
    "\n",
    "with open('binary_target.pickle','rb') as f:\n",
    "    binary_target = pickle.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(random_state=SEED)\n",
    "logit.fit(data_sparse, binary_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=3, penalty='l2', random_state=42, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_pres = LogisticRegression(random_state=SEED,multi_class='multinomial',solver='lbfgs',n_jobs=3)\n",
    "logit_pres.fit(data_sparse[np.where(binary_target == 'present')[0],:],target[binary_target[binary_target == 'present'].index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('../models/article_logit_binary.pickle','wb') as f:\n",
    "#    pickle.dump(logit,f)\n",
    "    \n",
    "with open('../models/article_logit_type.pickle','wb') as f:\n",
    "    pickle.dump(logit_pres,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just in case - list of classifiers that support predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Андрей\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\Андрей\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Андрей\\Anaconda3\\lib\\site-packages\\sklearn\\lda.py:6: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n",
      "C:\\Users\\Андрей\\Anaconda3\\lib\\site-packages\\sklearn\\learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Андрей\\Anaconda3\\lib\\site-packages\\sklearn\\qda.py:6: DeprecationWarning: qda.QDA has been moved to discriminant_analysis.QuadraticDiscriminantAnalysis in 0.17 and will be removed in 0.19.\n",
      "  \"in 0.17 and will be removed in 0.19.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier\n",
      "BaggingClassifier\n",
      "BayesianGaussianMixture\n",
      "BernoulliNB\n",
      "CalibratedClassifierCV\n",
      "DPGMM\n",
      "DecisionTreeClassifier\n",
      "ExtraTreeClassifier\n",
      "ExtraTreesClassifier\n",
      "GMM\n",
      "GaussianMixture\n",
      "GaussianNB\n",
      "GaussianProcessClassifier\n",
      "GradientBoostingClassifier\n",
      "KNeighborsClassifier\n",
      "LDA\n",
      "LabelPropagation\n",
      "LabelSpreading\n",
      "LinearDiscriminantAnalysis\n",
      "LogisticRegression\n",
      "LogisticRegressionCV\n",
      "MLPClassifier\n",
      "MultinomialNB\n",
      "NuSVC\n",
      "QDA\n",
      "QuadraticDiscriminantAnalysis\n",
      "RandomForestClassifier\n",
      "SGDClassifier\n",
      "SVC\n",
      "VBGMM\n",
      "_BinaryGaussianProcessClassifierLaplace\n",
      "_ConstantPredictor\n",
      "_DPGMMBase\n",
      "_GMMBase\n",
      "_LDA\n",
      "_QDA\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.testing import all_estimators\n",
    "\n",
    "estimators = all_estimators()\n",
    "\n",
    "for name, class_ in estimators:\n",
    "    if hasattr(class_, 'predict_proba'):\n",
    "        print(name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
