{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix, lil_matrix\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from copy import deepcopy\n",
    "from string import punctuation\n",
    "import pickle\n",
    "\n",
    "SEED = 42\n",
    "punct = set(punctuation) | {'‘','’','—',' ','\\t','\\n'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NP</th>\n",
       "      <th>POS_tags</th>\n",
       "      <th>Head</th>\n",
       "      <th>Head_countability</th>\n",
       "      <th>Head_POS</th>\n",
       "      <th>hypernyms</th>\n",
       "      <th>higher_hypernyms</th>\n",
       "      <th>HHead</th>\n",
       "      <th>HHead_POS</th>\n",
       "      <th>HHead_rel</th>\n",
       "      <th>...</th>\n",
       "      <th>post_2</th>\n",
       "      <th>post_3</th>\n",
       "      <th>post_4</th>\n",
       "      <th>post_5</th>\n",
       "      <th>post_1_POS</th>\n",
       "      <th>post_2_POS</th>\n",
       "      <th>post_3_POS</th>\n",
       "      <th>post_4_POS</th>\n",
       "      <th>post_5_POS</th>\n",
       "      <th>Preposition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is aids</td>\n",
       "      <td>DT NN</td>\n",
       "      <td>aids</td>\n",
       "      <td>both</td>\n",
       "      <td>NN</td>\n",
       "      <td>immunodeficiency infectious_disease</td>\n",
       "      <td>disorder disease</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aids</td>\n",
       "      <td>NN</td>\n",
       "      <td>aids</td>\n",
       "      <td>both</td>\n",
       "      <td>NN</td>\n",
       "      <td>immunodeficiency infectious_disease</td>\n",
       "      <td>disorder disease</td>\n",
       "      <td>condition</td>\n",
       "      <td>NN</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>...</td>\n",
       "      <td>acquired</td>\n",
       "      <td>immune</td>\n",
       "      <td>deficiency</td>\n",
       "      <td>syndrome</td>\n",
       "      <td>-LRB-</td>\n",
       "      <td>VBN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>NNP</td>\n",
       "      <td>NNP</td>\n",
       "      <td>zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>immune deficiency syndrome</td>\n",
       "      <td>NNP NNP NNP</td>\n",
       "      <td>syndrome</td>\n",
       "      <td>C</td>\n",
       "      <td>NNP</td>\n",
       "      <td>complex</td>\n",
       "      <td>concept</td>\n",
       "      <td>aids</td>\n",
       "      <td>NN</td>\n",
       "      <td>appos</td>\n",
       "      <td>...</td>\n",
       "      <td>is</td>\n",
       "      <td>a</td>\n",
       "      <td>condition</td>\n",
       "      <td>caused</td>\n",
       "      <td>-RRB-</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>DT</td>\n",
       "      <td>NN</td>\n",
       "      <td>VBN</td>\n",
       "      <td>zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a condition</td>\n",
       "      <td>DT NN</td>\n",
       "      <td>condition</td>\n",
       "      <td>C</td>\n",
       "      <td>NN</td>\n",
       "      <td>state</td>\n",
       "      <td>abstraction</td>\n",
       "      <td>be</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>cop</td>\n",
       "      <td>...</td>\n",
       "      <td>by</td>\n",
       "      <td>a</td>\n",
       "      <td>virus</td>\n",
       "      <td>called</td>\n",
       "      <td>VBN</td>\n",
       "      <td>IN</td>\n",
       "      <td>DT</td>\n",
       "      <td>NN</td>\n",
       "      <td>VBN</td>\n",
       "      <td>zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a virus</td>\n",
       "      <td>DT NN</td>\n",
       "      <td>virus</td>\n",
       "      <td>C</td>\n",
       "      <td>NN</td>\n",
       "      <td>infectious_agent microorganism</td>\n",
       "      <td>causal_agent living_thing</td>\n",
       "      <td>cause</td>\n",
       "      <td>VBN</td>\n",
       "      <td>obl</td>\n",
       "      <td>...</td>\n",
       "      <td>hiv</td>\n",
       "      <td>(</td>\n",
       "      <td>human</td>\n",
       "      <td>immuno</td>\n",
       "      <td>VBN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>-LRB-</td>\n",
       "      <td>NNP</td>\n",
       "      <td>NNP</td>\n",
       "      <td>by</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           NP     POS_tags       Head Head_countability  \\\n",
       "0                     is aids        DT NN       aids              both   \n",
       "1                        aids           NN       aids              both   \n",
       "2  immune deficiency syndrome  NNP NNP NNP   syndrome                 C   \n",
       "3                 a condition        DT NN  condition                 C   \n",
       "4                     a virus        DT NN      virus                 C   \n",
       "\n",
       "  Head_POS                            hypernyms           higher_hypernyms  \\\n",
       "0       NN  immunodeficiency infectious_disease           disorder disease   \n",
       "1       NN  immunodeficiency infectious_disease           disorder disease   \n",
       "2      NNP                              complex                    concept   \n",
       "3       NN                                state                abstraction   \n",
       "4       NN       infectious_agent microorganism  causal_agent living_thing   \n",
       "\n",
       "       HHead HHead_POS HHead_rel     ...        post_2  post_3      post_4  \\\n",
       "0                                    ...                                     \n",
       "1  condition        NN     nsubj     ...      acquired  immune  deficiency   \n",
       "2       aids        NN     appos     ...            is       a   condition   \n",
       "3         be       VBZ       cop     ...            by       a       virus   \n",
       "4      cause       VBN       obl     ...           hiv       (       human   \n",
       "\n",
       "     post_5 post_1_POS post_2_POS post_3_POS post_4_POS post_5_POS Preposition  \n",
       "0                    .                                                    zero  \n",
       "1  syndrome      -LRB-        VBN        NNP        NNP        NNP        zero  \n",
       "2    caused      -RRB-        VBZ         DT         NN        VBN        zero  \n",
       "3    called        VBN         IN         DT         NN        VBN        zero  \n",
       "4    immuno        VBN        NNP      -LRB-        NNP        NNP          by  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../preposition_table/shrinked_prepositions.csv',delimiter=';',encoding='utf-8-sig',keep_default_na=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000000, 31)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/one_word_vectorizer.pickle','rb') as f:\n",
    "    onewordvect = pickle.load(f)\n",
    "\n",
    "with open('../models/pos_vectorizer.pickle','rb') as f:\n",
    "    pos_vect = pickle.load(f)\n",
    "    \n",
    "with open('../models/noun_hypernym_vectorizer.pickle','rb') as f:\n",
    "    hyp_vect = pickle.load(f)\n",
    "            \n",
    "with open('../models/noun_higher_hypernym_vectorizer.pickle','rb') as f:\n",
    "    hhyp_vect = pickle.load(f)\n",
    "\n",
    "with open('../models/countability_vectorizer.pickle','rb') as f:\n",
    "    count_vect = pickle.load(f)\n",
    "    \n",
    "with open('../models/deprel_vectorizer.pickle','rb') as f:\n",
    "    deprel_vect = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('target.pickle','wb') as f:\n",
    "    pickle.dump(data['Preposition'],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['Preposition']\n",
    "target[target != 'zero'] = 'present'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('binary_target.pickle','wb') as f:\n",
    "    pickle.dump(target,f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_vect = CountVectorizer(token_pattern = '\\\\b\\\\w+\\\\b')\n",
    "npm = np_vect.fit_transform(data['NP'])\n",
    "\n",
    "pos = pos_vect.transform(data['POS_tags'])\n",
    "head_pos = pos_vect.transform(data['Head_POS'])\n",
    "hhead_pos = pos_vect.transform(data['HHead_POS'])\n",
    "prevs_pos = hstack([pos_vect.transform(data['prev_'+str(i)+'_POS']) for i in range(1,6)])\n",
    "posts_pos = hstack([pos_vect.transform(data['post_'+str(i)+'_POS']) for i in range(1,6)])\n",
    "\n",
    "\n",
    "countability = count_vect.transform(data['Head_countability'])\n",
    "\n",
    "\n",
    "hyp = hyp_vect.transform(data['hypernyms'])\n",
    "hhyp = hhyp_vect.transform(data['higher_hypernyms'])\n",
    "\n",
    "deprel = deprel_vect.transform(data['HHead_rel'])\n",
    "\n",
    "\n",
    "hhead_vect = CountVectorizer(token_pattern='.+')\n",
    "hhead = hhead_vect.fit_transform(data['HHead'])\n",
    "\n",
    "\n",
    "head = onewordvect.transform(data['Head'])\n",
    "prevs = hstack([onewordvect.transform(data['prev_'+str(i)]) for i in range(1,6)])\n",
    "posts = hstack([onewordvect.transform(data['post_'+str(i)]) for i in range(1,6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('npm.pickle','wb') as f:\n",
    "    pickle.dump(npm,f)\n",
    "    \n",
    "with open('pos.pickle','wb') as f:\n",
    "    pickle.dump(pos,f)\n",
    "    \n",
    "with open('head.pickle','wb') as f:\n",
    "    pickle.dump(head,f)\n",
    "    \n",
    "with open('countability.pickle','wb') as f:\n",
    "    pickle.dump(countability,f)\n",
    "    \n",
    "    \n",
    "with open('head_pos.pickle','wb') as f:\n",
    "    pickle.dump(head_pos,f)\n",
    "    \n",
    "with open('hyp.pickle','wb') as f:\n",
    "    pickle.dump(hyp,f)\n",
    "    \n",
    "with open('hhyp.pickle','wb') as f:\n",
    "    pickle.dump(hhyp,f)\n",
    "    \n",
    "with open('deprel.pickle','wb') as f:\n",
    "    pickle.dump(deprel,f)\n",
    "    \n",
    "with open('hhead.pickle','wb') as f:\n",
    "    pickle.dump(hhead,f)\n",
    "    \n",
    "with open('hhead_pos.pickle','wb') as f:\n",
    "    pickle.dump(hhead_pos,f)\n",
    "    \n",
    "with open('prevs.pickle','wb') as f:\n",
    "    pickle.dump(prevs,f)\n",
    "    \n",
    "with open('posts.pickle','wb') as f:\n",
    "    pickle.dump(posts,f)\n",
    "\n",
    "with open('prevs_pos.pickle','wb') as f:\n",
    "    pickle.dump(prevs_pos,f)\n",
    "\n",
    "with open('posts_pos.pickle','wb') as f:\n",
    "    pickle.dump(posts_pos,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/hhead_vectorizer.pickle','wb') as f:\n",
    "    pickle.dump(hhead_vect,f)\n",
    "    \n",
    "with open('../models/extended_np_vectorizer.pickle','wb') as f:\n",
    "    pickle.dump(np_vect,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTERMEDIATE STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('npm.pickle','rb') as f:\n",
    "    npm = pickle.load(f)\n",
    "    \n",
    "with open('pos.pickle','rb') as f:\n",
    "    pos = pickle.load(f)\n",
    "    \n",
    "with open('head.pickle','rb') as f:\n",
    "    head = pickle.load(f)\n",
    "    \n",
    "with open('countability.pickle','rb') as f:\n",
    "    countability = pickle.load(f)\n",
    "    \n",
    "with open('head_pos.pickle','rb') as f:\n",
    "    head_pos = pickle.load(f)\n",
    "    \n",
    "with open('hyp.pickle','rb') as f:\n",
    "    hyp = pickle.load(f)\n",
    "    \n",
    "with open('hhyp.pickle','rb') as f:\n",
    "    hhyp = pickle.load(f)\n",
    "\n",
    "with open('hhead.pickle','rb') as f:\n",
    "    hhead = pickle.load(f)\n",
    "    \n",
    "with open('hhead_pos.pickle','rb') as f:\n",
    "    hhead_pos = pickle.load(f)\n",
    "    \n",
    "with open('deprel.pickle','rb') as f:\n",
    "    deprel = pickle.load(f)\n",
    "    \n",
    "with open('prevs.pickle','rb') as f:\n",
    "    prevs = pickle.load(f)\n",
    "    \n",
    "with open('posts.pickle','rb') as f:\n",
    "    posts = pickle.load(f)\n",
    "\n",
    "with open('prevs_pos.pickle','rb') as f:\n",
    "    prevs_pos = pickle.load(f)\n",
    "\n",
    "with open('posts_pos.pickle','rb') as f:\n",
    "    posts_pos = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sparse = hstack((npm,pos,head,countability,head_pos,hyp,hhyp,hhead,hhead_pos,deprel,\n",
    "                      prevs,prevs_pos,posts,posts_pos)).tocsr()\n",
    "nonzero_columns = np.unique(data_sparse.nonzero()[1]) # TODO: need to remember what cols were omitted\n",
    "data_sparse = data_sparse[:,nonzero_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sparse_data.pickle','wb') as f:\n",
    "    pickle.dump(data_sparse,f)\n",
    "\n",
    "with open('../models/preposition_nonzero_columns.pickle','wb') as f:\n",
    "    pickle.dump(nonzero_columns,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000000, 1165020)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# presence classifier & a-an-the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_sparse, target, test_size=0.33, \n",
    "                                                    random_state=SEED,stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(random_state=SEED)\n",
    "logit.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.948183041723\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    present       0.96      0.92      0.94      6942\n",
      "       zero       0.94      0.97      0.96      9404\n",
      "\n",
      "avg / total       0.95      0.95      0.95     16346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_l = logit.predict(X_test)\n",
    "print(accuracy_score(y_test,pred_l))\n",
    "print(classification_report(y_test,pred_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_pres = LogisticRegression(random_state=SEED)\n",
    "logit_pres.fit(X_train[np.where(y_train == 'present')[0],:],target[y_train[y_train == 'present'].index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.564988730278\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      about       0.46      0.14      0.21        81\n",
      "      among       0.00      0.00      0.00        15\n",
      "         as       0.62      0.56      0.58       257\n",
      "         at       0.47      0.33      0.39       226\n",
      "    between       0.33      0.03      0.06        31\n",
      "         by       0.46      0.54      0.50       283\n",
      "     during       0.00      0.00      0.00        27\n",
      "        for       0.46      0.38      0.42       580\n",
      "       from       0.52      0.20      0.29       228\n",
      "         in       0.52      0.62      0.57      1190\n",
      "       into       0.39      0.16      0.23        73\n",
      "         of       0.67      0.91      0.77      2058\n",
      "         on       0.51      0.37      0.43       366\n",
      "       over       0.38      0.11      0.17        55\n",
      "         to       0.44      0.47      0.45       558\n",
      "       with       0.43      0.29      0.34       347\n",
      "       zero       0.00      0.00      0.00       280\n",
      "\n",
      "avg / total       0.52      0.56      0.53      6655\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC1\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pred_l_pres = logit_pres.predict(X_test[np.where(pred_l == 'present')[0],:])\n",
    "print(accuracy_score(target[y_test[pred_l == 'present'].index],pred_l_pres))\n",
    "print(classification_report(target[y_test[pred_l == 'present'].index],pred_l_pres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_l[pred_l == 'present'] = pred_l_pres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.788205065459\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      about       0.46      0.12      0.19        89\n",
      "      among       0.00      0.00      0.00        17\n",
      "         as       0.62      0.43      0.51       330\n",
      "         at       0.47      0.28      0.36       260\n",
      "    between       0.33      0.03      0.06        31\n",
      "         by       0.46      0.47      0.47       322\n",
      "     during       0.00      0.00      0.00        31\n",
      "        for       0.46      0.36      0.40       613\n",
      "       from       0.52      0.18      0.27       246\n",
      "         in       0.52      0.55      0.54      1325\n",
      "       into       0.39      0.16      0.23        74\n",
      "         of       0.67      0.89      0.76      2099\n",
      "         on       0.51      0.34      0.41       405\n",
      "       over       0.38      0.08      0.13        74\n",
      "         to       0.44      0.41      0.43       636\n",
      "       with       0.43      0.25      0.32       390\n",
      "       zero       0.94      0.97      0.96      9404\n",
      "\n",
      "avg / total       0.77      0.79      0.77     16346\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC1\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(target[y_test.index],pred_l))\n",
    "print(classification_report(target[y_test.index],pred_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit and save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sparse_data.pickle','rb') as f:\n",
    "    data_sparse = pickle.load(f)\n",
    "    \n",
    "with open('target.pickle','rb') as f:\n",
    "    target = pickle.load(f)\n",
    "\n",
    "with open('binary_target.pickle','rb') as f:\n",
    "    binary_target = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logit = LogisticRegression(random_state=SEED)\n",
    "logit.fit(data_sparse, binary_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_pres = LogisticRegression(random_state=SEED)\n",
    "logit_pres.fit(data_sparse[np.where(binary_target == 'present')[0],:],target[binary_target[binary_target == 'present'].index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/preposition_logit_binary.pickle','wb') as f:\n",
    "    pickle.dump(logit,f)\n",
    "    \n",
    "with open('../models/preposition_logit_type.pickle','wb') as f:\n",
    "    pickle.dump(logit_pres,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just in case - list of classifiers that support predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Андрей\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\Андрей\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Андрей\\Anaconda3\\lib\\site-packages\\sklearn\\lda.py:6: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n",
      "C:\\Users\\Андрей\\Anaconda3\\lib\\site-packages\\sklearn\\learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Андрей\\Anaconda3\\lib\\site-packages\\sklearn\\qda.py:6: DeprecationWarning: qda.QDA has been moved to discriminant_analysis.QuadraticDiscriminantAnalysis in 0.17 and will be removed in 0.19.\n",
      "  \"in 0.17 and will be removed in 0.19.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier\n",
      "BaggingClassifier\n",
      "BayesianGaussianMixture\n",
      "BernoulliNB\n",
      "CalibratedClassifierCV\n",
      "DPGMM\n",
      "DecisionTreeClassifier\n",
      "ExtraTreeClassifier\n",
      "ExtraTreesClassifier\n",
      "GMM\n",
      "GaussianMixture\n",
      "GaussianNB\n",
      "GaussianProcessClassifier\n",
      "GradientBoostingClassifier\n",
      "KNeighborsClassifier\n",
      "LDA\n",
      "LabelPropagation\n",
      "LabelSpreading\n",
      "LinearDiscriminantAnalysis\n",
      "LogisticRegression\n",
      "LogisticRegressionCV\n",
      "MLPClassifier\n",
      "MultinomialNB\n",
      "NuSVC\n",
      "QDA\n",
      "QuadraticDiscriminantAnalysis\n",
      "RandomForestClassifier\n",
      "SGDClassifier\n",
      "SVC\n",
      "VBGMM\n",
      "_BinaryGaussianProcessClassifierLaplace\n",
      "_ConstantPredictor\n",
      "_DPGMMBase\n",
      "_GMMBase\n",
      "_LDA\n",
      "_QDA\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.testing import all_estimators\n",
    "\n",
    "estimators = all_estimators()\n",
    "\n",
    "for name, class_ in estimators:\n",
    "    if hasattr(class_, 'predict_proba'):\n",
    "        print(name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
