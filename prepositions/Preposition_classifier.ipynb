{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC1\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix, lil_matrix\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from copy import deepcopy\n",
    "from string import punctuation\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import pickle\n",
    "\n",
    "SEED = 42\n",
    "punct = set(punctuation) | {'‘','’','—',' ','\\t','\\n'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(\"C:/Users/PC1/Desktop/python/деплом/deplom/constructions/GoogleNews-vectors-negative300.bin.gz\",\n",
    "                                      binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>raw_NP</th>\n",
       "      <th>NP</th>\n",
       "      <th>Start_idx</th>\n",
       "      <th>Sent_start_idx</th>\n",
       "      <th>POS_tags</th>\n",
       "      <th>Head</th>\n",
       "      <th>Head_countability</th>\n",
       "      <th>Head_POS</th>\n",
       "      <th>hypernyms</th>\n",
       "      <th>...</th>\n",
       "      <th>post_2</th>\n",
       "      <th>post_3</th>\n",
       "      <th>post_4</th>\n",
       "      <th>post_5</th>\n",
       "      <th>post_1_POS</th>\n",
       "      <th>post_2_POS</th>\n",
       "      <th>post_3_POS</th>\n",
       "      <th>post_4_POS</th>\n",
       "      <th>post_5_POS</th>\n",
       "      <th>Preposition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ageism</td>\n",
       "      <td>ageism</td>\n",
       "      <td>ageism</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NN</td>\n",
       "      <td>ageism</td>\n",
       "      <td></td>\n",
       "      <td>NN</td>\n",
       "      <td>discrimination</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the foundation of age discrimination</td>\n",
       "      <td>the foundation</td>\n",
       "      <td>the foundation</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>DT NN</td>\n",
       "      <td>foundation</td>\n",
       "      <td>both</td>\n",
       "      <td>NN</td>\n",
       "      <td>relation</td>\n",
       "      <td>...</td>\n",
       "      <td>age</td>\n",
       "      <td>discrimination</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>IN</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the foundation of age discrimination</td>\n",
       "      <td>of age discrimination</td>\n",
       "      <td>age discrimination</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>NN NN</td>\n",
       "      <td>discrimination</td>\n",
       "      <td>U</td>\n",
       "      <td>NN</td>\n",
       "      <td>social_control</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>steve scrutton</td>\n",
       "      <td>steve scrutton</td>\n",
       "      <td>steve scrutton</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>NNP NNP</td>\n",
       "      <td>scrutton</td>\n",
       "      <td></td>\n",
       "      <td>NNP</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Steve Scrutton is a social work manager in Nor...</td>\n",
       "      <td>Steve Scrutton</td>\n",
       "      <td>steve scrutton</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>NNP NNP</td>\n",
       "      <td>scrutton</td>\n",
       "      <td></td>\n",
       "      <td>NNP</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>a</td>\n",
       "      <td>social</td>\n",
       "      <td>work</td>\n",
       "      <td>manager</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>DT</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>zero</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence                 raw_NP  \\\n",
       "0                                             ageism                 ageism   \n",
       "1               the foundation of age discrimination         the foundation   \n",
       "2               the foundation of age discrimination  of age discrimination   \n",
       "3                                     steve scrutton         steve scrutton   \n",
       "4  Steve Scrutton is a social work manager in Nor...         Steve Scrutton   \n",
       "\n",
       "                   NP  Start_idx  Sent_start_idx POS_tags            Head  \\\n",
       "0              ageism          0               0       NN          ageism   \n",
       "1      the foundation          0               8    DT NN      foundation   \n",
       "2  age discrimination         15               8    NN NN  discrimination   \n",
       "3      steve scrutton          0              45  NNP NNP        scrutton   \n",
       "4      steve scrutton          0              60  NNP NNP        scrutton   \n",
       "\n",
       "  Head_countability Head_POS       hypernyms     ...     post_2  \\\n",
       "0                         NN  discrimination     ...              \n",
       "1              both       NN        relation     ...        age   \n",
       "2                 U       NN  social_control     ...              \n",
       "3                        NNP                     ...              \n",
       "4                        NNP                     ...          a   \n",
       "\n",
       "           post_3 post_4   post_5 post_1_POS post_2_POS post_3_POS post_4_POS  \\\n",
       "0                                                                               \n",
       "1  discrimination                         IN         NN         NN              \n",
       "2                                                                               \n",
       "3                                                                               \n",
       "4          social   work  manager        VBZ         DT         JJ         NN   \n",
       "\n",
       "  post_5_POS Preposition  \n",
       "0                   zero  \n",
       "1                   zero  \n",
       "2                     of  \n",
       "3                   zero  \n",
       "4         NN        zero  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('prepositions.csv',delimiter=';',encoding='utf-8-sig')\n",
    "data = data.fillna('')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Sentence', 'raw_NP', 'NP', 'Start_idx', 'Sent_start_idx', 'POS_tags',\n",
       "       'Head', 'Head_countability', 'Head_POS', 'hypernyms',\n",
       "       'higher_hypernyms', 'HHead', 'HHead_POS', 'HHead_rel', 'prev_5',\n",
       "       'prev_4', 'prev_3', 'prev_2', 'prev_1', 'prev_5_POS', 'prev_4_POS',\n",
       "       'prev_3_POS', 'prev_2_POS', 'prev_1_POS', 'post_1', 'post_2', 'post_3',\n",
       "       'post_4', 'post_5', 'post_1_POS', 'post_2_POS', 'post_3_POS',\n",
       "       'post_4_POS', 'post_5_POS', 'Preposition'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_vectors = lil_matrix((data.shape[0],300))\n",
    "for i,word in enumerate(data['Head']):\n",
    "    if word in model:\n",
    "        all_vectors[i,:] = model[word]\n",
    "        \n",
    "all_vectors_hhead = lil_matrix((data.shape[0],300))\n",
    "for i,word in enumerate(data['HHead']):\n",
    "    if word in model:\n",
    "        all_vectors_hhead[i,:] = model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50249x300 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 14090400 stored elements in LInked List format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50249x300 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 12167100 stored elements in LInked List format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vectors_hhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../models/one_word_vectorizer.pickle','rb') as f:\n",
    "    onewordvect = pickle.load(f)\n",
    "\n",
    "with open('../models/pos_vectorizer.pickle','rb') as f:\n",
    "    pos_vect = pickle.load(f)\n",
    "    \n",
    "with open('../models/noun_hypernym_vectorizer.pickle','rb') as f:\n",
    "    hyp_vect = pickle.load(f)\n",
    "            \n",
    "with open('../models/noun_higher_hypernym_vectorizer.pickle','rb') as f:\n",
    "    hhyp_vect = pickle.load(f)\n",
    "\n",
    "with open('../models/countability_vectorizer.pickle','rb') as f:\n",
    "    count_vect = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='.+', tokenizer=None,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../ud_relations.txt','r',encoding='utf-8') as f:\n",
    "    relations = f.read().split('\\n')\n",
    "\n",
    "deprel_vect = CountVectorizer(token_pattern='.+')\n",
    "deprel_vect.fit(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = data['Preposition']\n",
    "data = data.drop('Preposition',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pres_idx = target[target != 'zero'].index\n",
    "binary_target = deepcopy(target)\n",
    "binary_target[target != 'zero'] = 'present'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np_vect = CountVectorizer(token_pattern = '\\\\b\\\\w+\\\\b')\n",
    "npm = np_vect.fit_transform(data['NP'])\n",
    "\n",
    "pos = pos_vect.transform(data['POS_tags'])\n",
    "head_pos = pos_vect.transform(data['Head_POS'])\n",
    "hhead_pos = pos_vect.transform(data['HHead_POS'])\n",
    "prevs_pos = hstack([pos_vect.transform(data['prev_'+str(i)+'_POS']) for i in range(1,6)])\n",
    "posts_pos = hstack([pos_vect.transform(data['post_'+str(i)+'_POS']) for i in range(1,6)])\n",
    "\n",
    "countability = count_vect.transform(data['Head_countability'])\n",
    "\n",
    "hyp = hyp_vect.transform(data['hypernyms'])\n",
    "hhyp = hhyp_vect.transform(data['higher_hypernyms'])\n",
    "\n",
    "deprel = deprel_vect.transform(data['HHead_rel'])\n",
    "\n",
    "hhead_vect = CountVectorizer(token_pattern='.+')\n",
    "hhead = hhead_vect.fit_transform(data['HHead'])\n",
    "\n",
    "head = onewordvect.transform(data['Head'])\n",
    "prevs = hstack([onewordvect.transform(data['prev_'+str(i)]) for i in range(1,6)])\n",
    "posts = hstack([onewordvect.transform(data['post_'+str(i)]) for i in range(1,6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sparse = hstack((npm,pos,head,countability,head_pos,hyp,hhyp,all_vectors,hhead,hhead_pos,deprel,all_vectors_hhead,\n",
    "                      prevs,prevs_pos,posts,posts_pos)).tocsr()\n",
    "#nonzero_columns = np.unique(data_sparse.nonzero()[1]) # TODO: need to remember what cols were omitted\n",
    "#data_sparse = data_sparse[:,nonzero_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50249, 1379745)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50249x300 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 14090400 stored elements in LInked List format>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# presence classifier & a-an-the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_sparse, binary_target, test_size=0.33, \n",
    "                                                    random_state=SEED,stratify=binary_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(random_state=SEED)\n",
    "logit.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.948561780136\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    present       0.95      0.92      0.93      6528\n",
      "       zero       0.95      0.97      0.96     10055\n",
      "\n",
      "avg / total       0.95      0.95      0.95     16583\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_l = logit.predict(X_test)\n",
    "print(accuracy_score(y_test,pred_l))\n",
    "print(classification_report(y_test,pred_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_pres = LogisticRegression(random_state=SEED)\n",
    "logit_pres.fit(X_train[np.where(y_train == 'present')[0],:],target[y_train[y_train == 'present'].index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.588057805304\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      about       0.40      0.16      0.23        89\n",
      "      along       0.00      0.00      0.00         1\n",
      "      among       0.00      0.00      0.00        11\n",
      "     around       0.00      0.00      0.00         6\n",
      "         as       0.65      0.56      0.60       248\n",
      "         at       0.51      0.44      0.47       224\n",
      "    between       0.86      0.18      0.30        33\n",
      "         by       0.48      0.50      0.49       286\n",
      "       down       0.00      0.00      0.00         5\n",
      "     during       0.00      0.00      0.00        24\n",
      "     except       0.00      0.00      0.00         1\n",
      "        for       0.47      0.46      0.46       591\n",
      "       from       0.42      0.27      0.33       222\n",
      "         in       0.53      0.66      0.59      1167\n",
      "     inside       0.00      0.00      0.00         1\n",
      "       into       0.40      0.24      0.30        84\n",
      "         of       0.71      0.90      0.79      2087\n",
      "        off       0.00      0.00      0.00         8\n",
      "         on       0.50      0.45      0.48       370\n",
      "       onto       0.00      0.00      0.00         5\n",
      "    outside       0.00      0.00      0.00         9\n",
      "       over       0.32      0.15      0.21        53\n",
      "    through       0.00      0.00      0.00        27\n",
      "    towards       0.00      0.00      0.00        15\n",
      "      under       0.74      0.44      0.55        32\n",
      "      until       1.00      0.08      0.15        12\n",
      "       upon       0.00      0.00      0.00         5\n",
      "       with       0.41      0.35      0.38       324\n",
      "     within       0.50      0.04      0.07        25\n",
      "    without       0.50      0.05      0.09        21\n",
      "       zero       0.00      0.00      0.00       311\n",
      "\n",
      "avg / total       0.53      0.59      0.55      6297\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC1\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pred_l_pres = logit_pres.predict(X_test[np.where(pred_l == 'present')[0],:])\n",
    "print(accuracy_score(target[y_test[pred_l == 'present'].index],pred_l_pres))\n",
    "print(classification_report(target[y_test[pred_l == 'present'].index],pred_l_pres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_l[pred_l == 'present'] = pred_l_pres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.810890671169\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      about       0.40      0.13      0.20       105\n",
      "      along       0.00      0.00      0.00         2\n",
      "      among       0.00      0.00      0.00        16\n",
      "     around       0.00      0.00      0.00         9\n",
      "         as       0.65      0.44      0.52       320\n",
      "         at       0.51      0.38      0.44       257\n",
      "    between       0.86      0.17      0.29        35\n",
      "         by       0.48      0.45      0.46       318\n",
      "       down       0.00      0.00      0.00         5\n",
      "     during       0.00      0.00      0.00        27\n",
      "     except       0.00      0.00      0.00         1\n",
      "        for       0.47      0.43      0.44       641\n",
      "       from       0.42      0.24      0.31       243\n",
      "         in       0.53      0.60      0.56      1298\n",
      "     inside       0.00      0.00      0.00         1\n",
      "       into       0.40      0.23      0.29        87\n",
      "         of       0.71      0.88      0.79      2128\n",
      "        off       0.00      0.00      0.00         9\n",
      "         on       0.50      0.41      0.45       413\n",
      "       onto       0.00      0.00      0.00         5\n",
      "    outside       0.00      0.00      0.00        12\n",
      "       over       0.32      0.13      0.18        62\n",
      "    through       0.00      0.00      0.00        37\n",
      "    towards       0.00      0.00      0.00        15\n",
      "      under       0.74      0.39      0.51        36\n",
      "      until       1.00      0.06      0.12        16\n",
      "       upon       0.00      0.00      0.00         8\n",
      "       with       0.41      0.31      0.35       370\n",
      "     within       0.50      0.03      0.06        29\n",
      "    without       0.50      0.04      0.08        23\n",
      "       zero       0.95      0.97      0.96     10055\n",
      "\n",
      "avg / total       0.79      0.81      0.80     16583\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC1\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(target[y_test.index],pred_l))\n",
    "print(classification_report(target[y_test.index],pred_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit and save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(random_state=SEED)\n",
    "logit.fit(data_sparse, binary_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_pres = LogisticRegression(random_state=SEED)\n",
    "logit_pres.fit(data_sparse[np.where(binary_target == 'present')[0],:],target[binary_target[binary_target == 'present'].index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../models/hhead_vectorizer.pickle','wb') as f:\n",
    "    pickle.dump(hhead_vect,f)\n",
    "    \n",
    "with open('../models/deprel_vectorizer.pickle','wb') as f:\n",
    "    pickle.dump(deprel_vect,f)\n",
    "    \n",
    "with open('../models/extended_np_vectorizer.pickle','wb') as f:\n",
    "    pickle.dump(np_vect,f)\n",
    "\n",
    "\n",
    "\n",
    "with open('../models/preposition_logit_binary.pickle','wb') as f:\n",
    "    pickle.dump(logit,f)\n",
    "    \n",
    "with open('../models/preposition_logit_type.pickle','wb') as f:\n",
    "    pickle.dump(logit_pres,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just in case - list of classifiers that support predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Андрей\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\Андрей\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Андрей\\Anaconda3\\lib\\site-packages\\sklearn\\lda.py:6: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n",
      "C:\\Users\\Андрей\\Anaconda3\\lib\\site-packages\\sklearn\\learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Андрей\\Anaconda3\\lib\\site-packages\\sklearn\\qda.py:6: DeprecationWarning: qda.QDA has been moved to discriminant_analysis.QuadraticDiscriminantAnalysis in 0.17 and will be removed in 0.19.\n",
      "  \"in 0.17 and will be removed in 0.19.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier\n",
      "BaggingClassifier\n",
      "BayesianGaussianMixture\n",
      "BernoulliNB\n",
      "CalibratedClassifierCV\n",
      "DPGMM\n",
      "DecisionTreeClassifier\n",
      "ExtraTreeClassifier\n",
      "ExtraTreesClassifier\n",
      "GMM\n",
      "GaussianMixture\n",
      "GaussianNB\n",
      "GaussianProcessClassifier\n",
      "GradientBoostingClassifier\n",
      "KNeighborsClassifier\n",
      "LDA\n",
      "LabelPropagation\n",
      "LabelSpreading\n",
      "LinearDiscriminantAnalysis\n",
      "LogisticRegression\n",
      "LogisticRegressionCV\n",
      "MLPClassifier\n",
      "MultinomialNB\n",
      "NuSVC\n",
      "QDA\n",
      "QuadraticDiscriminantAnalysis\n",
      "RandomForestClassifier\n",
      "SGDClassifier\n",
      "SVC\n",
      "VBGMM\n",
      "_BinaryGaussianProcessClassifierLaplace\n",
      "_ConstantPredictor\n",
      "_DPGMMBase\n",
      "_GMMBase\n",
      "_LDA\n",
      "_QDA\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.testing import all_estimators\n",
    "\n",
    "estimators = all_estimators()\n",
    "\n",
    "for name, class_ in estimators:\n",
    "    if hasattr(class_, 'predict_proba'):\n",
    "        print(name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
